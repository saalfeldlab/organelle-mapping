import logging

import numpy as np
import torch
from numpy.typing import NDArray

from fly_organelles.model import load_eval_model

logger = logging.getLogger(__name__)

CHECKPOINT_PATH = "/nrs/saalfeld/heinrichl/fly_organelles/run08/model_checkpoint_438000"
NUM_OUTPUTS = 8  # 0:all_mem,1:organelle,2:mito,3:er,4:nucleus,5:pm,6:vs,7:ld


def single_block_inference(
    raw_arrs: list[NDArray],
    min_raw: int | float | list[int | float | None] | None = None,
    max_raw: int | float | list[int | float | None] | None = None,
    batched_mode: bool = False,
) -> list[NDArray[np.float32]]:
    """Run predictions for a few single blocks.

    Args:
        raw_arrs (list[NDArray]): List of numpy arrays for which to run prediction.
            Expected shape for each: (130+nz*8, 130+ny*8, 130+nx*8) with nx, ny, nz
            integers. For batched_mode all arrays need to have the same shape.
        min_raw (int |  float | list[int | float  |  None] | None, optional): Contrast
            is adjusted such that `min_raw` is mapped to -1 for network input. If None,
            this value is inferred from the data type. Value may be specified per array.
            Defaults to None.
        max_raw (int | float | list[int |float  |  None] | None, optional): Contrast is
            adjusted such that `max_raw` is mapped to 1 for network input. If None,
            this value is inferred from the data type. Value may be specified per array.
            Defaults to None.
        batched_mode (bool, optional): Whether the arrays in the list are predicted
            jointly as a batch. If False, they're run iteratively. Defaults to False.

    Returns:
        list[NDArray[np.float32]]: List of predictions generated by network. Shape of
            each: (NUM_OUTPUTS, 8+nz*8, 8+ny*8, 8+nx*8)
    """
    if min_raw is None or isinstance(min_raw, (float, int)):
        min_raw = [
            min_raw,
        ] * len(raw_arrs)
    if max_raw is None or isinstance(max_raw, (float, int)):
        max_raw = [
            max_raw,
        ] * len(raw_arrs)
    for raw_idx, (raw_arr, min_r, max_r) in enumerate(zip(raw_arrs, min_raw, max_raw)):
        # figure out normalization parameters
        if min_r is None:
            if issubclass(raw_arr.dtype.type, np.integer):
                min_r = np.iinfo(raw_arr.dtype).min
            else:
                min_r = 0.0
            logger.info(f"Inferred minimum value for input data as {min_r}")
        if max_r is None:
            if issubclass(raw_arr.dtype.type, np.integer):
                max_r = np.iinfo(raw_arr.dtype).max
            else:
                max_r = 1.0
            logger.info(f"Inferred maximum value for input data as {max_r}")
        shift = min_r
        scale = max_r - min_r

        # Shift and scale map data s.t. min_raw -> 0, max_raw -> 1
        # Then shift again s.t. min_raw -> -1, max_raw -> 1
        raw_arrs[raw_idx] = 2 * (raw_arr.astype(np.float32) - shift) / scale - 1.0

        # add batch and channel dimensions
        raw_arrs[raw_idx] = np.expand_dims(raw_arrs[raw_idx], (0, 1))

    # load model
    model = load_eval_model(NUM_OUTPUTS, CHECKPOINT_PATH)

    # Find out device to which model was loaded so that tensor can be put on the same
    # device
    device = next(model.parameters()).device
    logger.info(f"Running on device: {device}")

    # Run the prediction
    with torch.inference_mode():  # This turns off gradient computation
        if batched_mode:  # run predictions iteratively
            input_tensor = torch.from_numpy(np.concat(raw_arrs, 0))
            predictions = (
                model.forward(input_tensor.float().to(device)).detach().cpu().numpy()
            )
            predictions = list(predictions)
        else:  # run predictions iteratively
            predictions = []
            for inp in raw_arrs:
                input_tensor = torch.from_numpy(inp)
                predictions.append(
                    model.forward(input_tensor.float().to(device))
                    .detach()
                    .cpu()
                    .numpy()[0]
                )
    return predictions


if __name__ == "__main__":
    logger.setLevel(logging.INFO)
    raw_sim = [
        np.random.randint(0, 255, size=(130, 130, 130), dtype=np.uint8),
        np.random.randint(0, 255, size=(130, 130, 130), dtype=np.uint8),
    ]
    pred = single_block_inference(raw_sim, batched_mode=False)
    for ret in pred:
        logger.info(f"{ret.shape=}, {ret.min()=}, {ret.max()=}, {ret.dtype=}")
